import cv2
from Utils import imageProcessingUtil, modelDictionary, modelLoader, GUIController, AffectiveMemory
import numpy

import tensorflow as tf


class EmotionDetection:

    def __init__(self):
        print('++++++++++++++++++ Initialising Emotion Detection ++++++++++++++++++++++++')
        self.config = tf.ConfigProto()
        self.config.gpu_options.allow_growth = True

        self.graph = tf.get_default_graph()
        self.sess = tf.Session(config=self.config, graph=self.graph)


        self.finalImageSize = (1024, 768)  # Size of the final image generated by the demo
        self.categoricalInitialPosition = 460  # Initial position for adding the categorical graph in the final image
        self.faceSize = (64, 64)  # Input size for both models: categorical and dimensional
        self.faceDetectionMaximumFrequency = 20  # Frequency that a face will be detected: every X frames.

        self.affectiveMemory = AffectiveMemory.AffectiveMemory()  # Affective Memory

        self.modelDimensional = modelLoader.modelLoader(modelDictionary.DimensionalModel, session=self.sess)

        self.imageProcessing = imageProcessingUtil.imageProcessingUtil()

        self.GUIController = GUIController.GUIController()

        self.rolling_average_arousal = 0
        self.rolling_average_valence = 0



    def get_arousal_valence_for_image(self, frame):
        # print "inside emotion detection"
        # detect faces
        facePoints, face = self.imageProcessing.detectFace(frame)

        # create display image and copy the captured frame to it
        image = numpy.zeros((self.finalImageSize[1], self.finalImageSize[0], 3), numpy.uint8)

        # y = int((frame.shape[0] - 480) / 2)
        # x = int((frame.shape[1] - 720) / 2)
        # image[0:480, 0:640] = frame[y:y + 480, x:x + 640]
        image[0:480, 0:640] = frame
        frame = image

        # If a face is detected
        if not len(face) == 0:
            # pre-process the face
            face = self.imageProcessing.preProcess(face, self.faceSize)

            # cv2.imshow('frame', frame)
            #
            # if cv2.waitKey(1) & 0xFF == ord('q'):
            #     cv2.destroyAllWindows()

            # Obtain dimensional classification

            dimensionalRecognition = numpy.array(self.modelDimensional.classify(face, self.graph, self.sess))
            # print('-------------HERE-----------------')

            # ----------- Affective Memory ----------------------

            # Obtain the affective memory input / output of the dense layer of the network
            # affectiveMemoryInput = numpy.squeeze(numpy.array(modelDimensional.getDense(face)),1)

            # # Otherwise use arousal/valence as input to the affective memory
            affectiveMemoryInput = numpy.array(dimensionalRecognition[:, 0, 0]).flatten()

            # If affective memory is not built, build it.
            if not self.affectiveMemory.isBuilt:
                # print ("BUild")
                self.affectiveMemory.buildModel(affectiveMemoryInput)
            # if affective memory is already built, train it with the new facial expression
            else:
                # print("train")
                self.affectiveMemory.train(affectiveMemoryInput)

            # affectiveMemoryNodes, affectiveMemoryNodesAges = self.affectiveMemory.getNodes()

            arousal = 1 - float(float(dimensionalRecognition[0][0][0]) * 100)
            valence = float(float(dimensionalRecognition[1][0][0]) * 100)

            # print "Arousal:", arousal
            # print "Valence:", valence

        else:
            return [0, 0]
        # Display the resulting frame
        cv2.imshow('frame', frame)

        if cv2.waitKey(1) & 0xFF == ord('q'):
            cv2.destroyAllWindows()

        return arousal, valence



    def get_rolling_arousal_valence(self):
        return self.rolling_average_arousal, self.rolling_average_valence